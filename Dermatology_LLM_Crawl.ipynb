{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5c71342",
   "metadata": {},
   "source": [
    "# Crawl data from Google Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eff2ce6",
   "metadata": {},
   "source": [
    "## Make the dataset connected into Azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87e451d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# %pip install beautifulsoup4\n",
    "# %pip install openpyxl\n",
    "# !pip install selenium pillow\n",
    "# !pip install webdriver_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcab055",
   "metadata": {},
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39e3b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a25077c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Excel data\n",
    "ethnic_df = pd.read_excel(\"Datasets/Ethnic_Origin_Facial_Issues.xlsx\")\n",
    "issue_df = pd.read_excel(\"Datasets/Frequent_Facial_Issues.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8baf8088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a constant key and perform a cross join\n",
    "ethnic_df['key'] = 1\n",
    "issue_df['key'] = 1\n",
    "cross_joined_df = pd.merge(ethnic_df, issue_df, on='key').drop('key', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce067c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the first few rows\n",
    "cross_joined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72d7f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of the DataFrame\n",
    "cross_joined_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e1dc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract column names\n",
    "col_name = list(cross_joined_df.columns)\n",
    "col_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c865a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame with selected columns\n",
    "df = cross_joined_df[['Ethnic Origin', 'Issue']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06794774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column for search keywords\n",
    "df['key_word'] = df['Ethnic Origin'] + \"'s \" + df['Issue']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14128d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over rows to check keywords\n",
    "for index, row in df.iterrows():\n",
    "    print(row['key_word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c10a67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up directory for downloading images\n",
    "import os\n",
    "os.mkdir(\"/Workspace/Users/hanchen@845861076.onmicrosoft.com/Dermatology_LLM/images\")\n",
    "os.chdir(\"/Workspace/Users/hanchen@845861076.onmicrosoft.com/Dermatology_LLM/images\")\n",
    "print(os.getcwd())\n",
    "\n",
    "# Crawl images from Google\n",
    "for index, row in df.iterrows():\n",
    "    search_query = row['key_word']\n",
    "    url = f\"https://www.google.com/search?hl=en&tbm=isch&q={search_query}\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    images = soup.find_all('img')\n",
    "    image_urls = [img['src'] for img in images if img['src'].startswith('http')]\n",
    "\n",
    "    # Create a folder for each issue\n",
    "    if not os.path.exists(row['Issue']):\n",
    "        os.makedirs(search_query)\n",
    "\n",
    "    # Download first 10 images\n",
    "    count = 0\n",
    "    for img_url in image_urls:\n",
    "        if count >= 10:\n",
    "            break\n",
    "        try:\n",
    "            img_data = requests.get(img_url).content\n",
    "            img_size = len(img_data)\n",
    "            if img_size > 50000:\n",
    "                with open(f\"{search_query}/image_{count+1}.jpg\", 'wb') as f:\n",
    "                    f.write(img_data)\n",
    "                print(f\"Downloaded {search_query}/image_{count+1}.jpg\")\n",
    "                count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download image: {e}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
